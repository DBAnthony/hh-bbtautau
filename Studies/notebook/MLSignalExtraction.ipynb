{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "import pandas\n",
    "from sklearn import metrics # score functions and performance metrics\n",
    "from sklearn.model_selection import train_test_split # split training and testing datasets\n",
    "import keras # a high-level API to train and evaluate neural networks\n",
    "from keras.models import Model, load_model # NN Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, BatchNormalization # various layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping # callbacks used during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Samples:\n",
    "    Signal_NonRes = -125\n",
    "    Signal_Radion = [ -260, -270, -280, -300, -320, -340, -350,\n",
    "                      -400, -450, -500, -550, -600, -650, -750, -800, -900 ]\n",
    "    Data = 0\n",
    "    TT = 1\n",
    "    DY = 2\n",
    "    Wjets= 3\n",
    "    SM_Higgs = 4\n",
    "    other_bkg = 5\n",
    "    \n",
    "class btag_wp:\n",
    "    Loose = 0.5426\n",
    "    Medium = 0.8484\n",
    "    Tight = 0.9535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data. Select only branches with variables needed for the training in order to reduce the memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/eos/home-m/mgrippo/CMSDAS_2019_hh_bbtautau/anaTuples/\"\n",
    "path = \"./anaTuples/\"\n",
    "channel = \"tauTau\"\n",
    "\n",
    "branches = [ 'sample_id', 'region_id', 'm_sv', 'm_bb', 'm_ttbb_kinfit', 'csv_b*', 'weight' ]\n",
    "\n",
    "with uproot.open(path+channel+\"_tuple.root\") as file:\n",
    "    tree = file[channel]\n",
    "    all_branches = [ b.decode('utf-8') for b in tree.allkeys() ]\n",
    "    print(\"All available branches: \", all_branches)\n",
    "    df = tree.arrays(branches, outputtype=pandas.DataFrame)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for nonresonant sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define branche to indicate siganl or background and fix variable ranges. For example, when kinfit is not converged, put -1, instead of the default value (which is std::numeric_limits<float>::lowest())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_signal'] = pandas.Series((df.sample_id == Samples.Signal_NonRes).astype(int), df.index)\n",
    "df['is_bkg'] = pandas.Series((df.sample_id > 0).astype(int), df.index)\n",
    "df['m_ttbb_kinfit'] = pandas.Series(np.maximum(df.m_ttbb_kinfit, -1), df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to apply correct weights to give correct importance for different backgrounds. On the other hand, signal and background should have the same total weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_weight = (df.weight * df.is_signal / np.sum(df.weight * df.is_signal) \\\n",
    "            + df.weight * df.is_bkg / np.sum(df.weight * df.is_bkg)) * df[(df.is_signal == 1) | (df.is_bkg == 1)].shape[0]\n",
    "df['ml_weight'] = pandas.Series(ml_weight, df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input branches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_branches = ['m_sv', 'm_bb', 'm_ttbb_kinfit']\n",
    "input_shape = (len(input_branches), )\n",
    "print(\"Number of inputs: \", len(input_branches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select only signal and background events;\n",
    "- shuffle the dataset\n",
    "- split the original dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[(df.is_signal == 1) | (df.is_bkg == 1)]\n",
    "data_train, data_test = train_test_split(data.sample(frac=1.), test_size=0.2)\n",
    "X_train = data_train[input_branches]\n",
    "Y_train = data_train['is_signal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network model and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers = 5 # number of hidden layers\n",
    "n_neurons = len(input_branches) * 2 # number of neurons per layer\n",
    "dropout_rate = 0.5 # fraction of the neurons that are randomly dropped before each training step\n",
    "\n",
    "input_layer = Input(name=\"main_input\", shape=input_shape) # Input layer\n",
    "\n",
    "prev_layer = input_layer\n",
    "for n in range(n_hidden_layers):\n",
    "    # Fully connected dense layer. The weights are initialized with He uniform initializer, \n",
    "    # which samples weights from from an uniform distribution within [-limit, limit],\n",
    "    # where limit is sqrt(6 / n_inputs).\n",
    "    dense_layer = Dense(n_neurons, name=\"dense_%d\" % n,\n",
    "                        kernel_initializer='he_uniform')(prev_layer) \n",
    "    \n",
    "    # Apply a batch normalization layer to reduce dependency on the statistical fluctuations between the batchs.\n",
    "    # Batch normalization normalizes the output of a previous activation layer by subtracting\n",
    "    # the batch mean and dividing by the batch standard deviation.\n",
    "    layer = BatchNormalization(name=\"batch_norm_%d\" % n)(dense_layer)\n",
    "    \n",
    "    # Activation layer that is needed to introduce non-linearity to the NN response.\n",
    "    # Here ReLU activation function is used. ReLU(x) = max(0, x).\n",
    "    activation_layer = Activation('relu', name=\"activation_%d\" % n)(layer)\n",
    "    \n",
    "    # Layer that randomly drop part of the outputs before each training step.\n",
    "    prev_layer = Dropout(dropout_rate, name=\"dropout_%d\" % n)(activation_layer)\n",
    "    \n",
    "# Output layer and its activation. A signle output is used:\n",
    "# for background-like events we want to have values closer to 0,\n",
    "# while for signal-like events we want to have values closer to 1.\n",
    "output_layer = Dense(1, name=\"dense_%d\" % n_hidden_layers)(prev_layer)\n",
    "softmax_output = Activation(\"sigmoid\", name=\"main_output\")(output_layer)\n",
    "\n",
    "# Create model indicating input and output layers.\n",
    "model = Model(input_layer, softmax_output, name='bbtautau_model')\n",
    "\n",
    "# Optimizer that implements the Adam algorithm. See http://arxiv.org/abs/1412.6980\n",
    "# lr indicates the learning rate of the algorithm.\n",
    "# Here is the list of optimizers available in Keras: https://keras.io/optimizers/\n",
    "opt = keras.optimizers.Adam(lr=1e-3)\n",
    "\n",
    "# Compile the model. \n",
    "# Cross entropy is chosen as the loss function, while the accuracy is used as\n",
    "# an additional metric for cross-check purposes.\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback that stores the best network based on the validation loss.\n",
    "checkpoint = ModelCheckpoint('best_network.hdf5', monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "# Callback that stops the training, if the selected quantity does not improve for the given number of epochs.\n",
    "early_stopping = EarlyStopping('val_loss', patience=10)\n",
    "# Fit the model and store the history.\n",
    "# - 25% of the training set is used for the validation\n",
    "# - each batch contains 10k events\n",
    "# - total number of epochs is 1000, but should stop much earlier by the EarlyStopping callback\n",
    "history = model.fit(X_train.values, Y_train.values, sample_weight=data_train.ml_weight.values,\n",
    "                    callbacks=[checkpoint, early_stopping], validation_split=0.25,\n",
    "                    batch_size=10000, epochs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss for training and validation sets as a function of epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history['loss'], history.history['val_loss'])\n",
    "plt.legend(['training set', 'validation set'])\n",
    "plt.title('Loss vs epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best network and apply it to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_network.hdf5')\n",
    "pred = model.predict(data_test[input_branches].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(data_test['is_signal'], pred[:, 0], sample_weight=data_test.ml_weight.values)\n",
    "plt.plot(tpr, 1-fpr)\n",
    "plt.title('ROC curve', fontsize=20)\n",
    "plt.xlabel('Signal efficiency', fontsize=16)\n",
    "plt.ylabel('Background rejection', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "metrics.roc_auc_score(data_test['is_signal'], pred[:, 0], sample_weight=data_test.ml_weight.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand network performance\n",
    "- Add more variables.\n",
    "  - You can't use tau isolation, because it is used in the sideband definition.\n",
    "  - If you use csv as an input, you should apply at least the loose btag working point,\n",
    "    otherwise the agreement between data and MC is not guaranteed.\n",
    "- Evaluate the performance in the signal region against all and against the main background (ttbar).\n",
    "- Compare performance of the training that run only on the signal region events\n",
    "  (less stat, but the parameter space is exact as in the final selection).\n",
    "- Tune network parameters and see the effect on the performance:\n",
    "  - number of layers\n",
    "  - number of neurons per layer\n",
    "  - decreas number of neurons in each consecutive layer\n",
    "  - learning rate\n",
    "  - other minimizers: SGD, NAdam\n",
    "  - effect of dropout rate\n",
    "  - effect of batch normalization:\n",
    "    - train network with ReLU without the batch normalization\n",
    "    - implement self-normalizing network using SELU activation.\n",
    "      In that case the kernel_init should be lecun_normal and AlphaDropout should be used for dropout.\n",
    "- Use S/sqrt(S+B) estimator from the previous exercise to evaluate if the additional cut on the NN output improves the significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrized learning\n",
    "- Try to apply the training procedure established above discriminate the resonant signal.\n",
    "- Add gen mass as parameter and see if performance has improved.\n",
    "- Given one mass hypothesis, see how performance changes for the signal samples with the different mX.\n",
    "- Add production_mode parameter to be able to train both resonant and non-resonant together.\n",
    "- (advanced) add channel parameter, so all the channels can be trained together.\n",
    "\n",
    "Some code snippets are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is_signal = 1 for all Radion samples\n",
    "df['is_signal'] = pandas.Series(((df.sample_id < 0) & (df.sample_id != Samples.Signal_NonRes)).astype(int), df.index)\n",
    "df['is_bkg'] = pandas.Series((df.sample_id > 0).astype(int), df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new ml_weight. making sure that all mass points have the same total weight.\n",
    "ml_weight = df.weight * df.is_bkg / np.sum(df.weight * df.is_bkg)\n",
    "for mX in Samples.Signal_Radion:\n",
    "    ml_weight += df.weight * (df.sample_id == mX).astype(int) / np.sum(df.weight * (df.sample_id == mX).astype(int)) \\\n",
    "                 / len(Samples.Signal_Radion)\n",
    "ml_weight *= df[(df.is_signal == 1) | (df.is_bkg == 1)].shape[0]\n",
    "df['ml_weight'] = pandas.Series(ml_weight, df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create branch with mX at the gen level to be used as an input.\n",
    "# Set it to a random value for the backgrounds.\n",
    "mX = -df.sample_id * df.is_signal - df.is_bkg * np.random.choice(Samples.Signal_Radion, size=df.shape[0]) \n",
    "df['mX'] = pandas.Series(mX, df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mX to the list of the input branches\n",
    "input_branches = [ 'mX', ... ]\n",
    "input_shape = (len(input_branches), )\n",
    "print(\"Number of inputs: \", len(input_branches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in training and testing as before\n",
    "data = df[(df.is_signal == 1) | (df.is_bkg == 1)]\n",
    "data_train, data_test = train_test_split(data.sample(frac=1.), test_size=0.2)\n",
    "X_train = data_train[input_branches]\n",
    "Y_train = data_train['is_signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the NN model and compile it\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "#model = load_model('best_param_network.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set for each mX hypothesis.\n",
    "scores = []\n",
    "for sample_id in Samples.Signal_Radion:\n",
    "    mX = -sample_id\n",
    "    data_test_mX = data_test[(data_test.sample_id == sample_id) | (data_test.sample_id > 0)].copy()\n",
    "    data_test_mX['mX'] = pandas.Series(np.ones(data_test_mX.shape[0]) * mX, data_test_mX.index)\n",
    "    pred = model.predict(data_test_mX[input_branches].values)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(data_test_mX['is_signal'], pred[:, 0], sample_weight=data_test_mX.ml_weight.values)\n",
    "    plt.plot(tpr, 1-fpr)\n",
    "    plt.title('ROC curve for mX = {} GeV'.format(mX), fontsize=20)\n",
    "    plt.xlabel('Signal efficiency', fontsize=16)\n",
    "    plt.ylabel('Background rejection', fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    score = metrics.roc_auc_score(data_test_mX['is_signal'], pred[:, 0], sample_weight=data_test_mX.ml_weight.values)\n",
    "    print('mX = {} GeV, score = {}'.format(mX, score))\n",
    "    scores.append(score)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot auc score as a function of mX\n",
    "plt.plot(-np.array(Samples.Signal_Radion), scores)\n",
    "plt.title('auc score vs. mX')\n",
    "plt.xlabel('mX (GeV)')\n",
    "plt.ylabel('auc score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
